{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE1Gt7OfBiM4"
      },
      "source": [
        "# Import Libary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_2PtuFbiEsL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gdown\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "066rWDrlBlr6"
      },
      "source": [
        "# Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gobl414ekjz8",
        "outputId": "682c2588-4b43-4c62-ed01-feb9b2088a44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1e45f1GmiKKtHkbZPg8WkyjemBVWgjfOE\n",
            "From (redirected): https://drive.google.com/uc?id=1e45f1GmiKKtHkbZPg8WkyjemBVWgjfOE&confirm=t&uuid=2af2dcc0-7f95-409d-a583-16a60be42f9d\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 653M/653M [00:08<00:00, 78.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "file_id = \"1e45f1GmiKKtHkbZPg8WkyjemBVWgjfOE\"\n",
        "\n",
        "# URL berbagi dari Google Drive\n",
        "url = 'https://drive.google.com/uc?id=' + file_id\n",
        "\n",
        "# Path untuk menyimpan file yang diunduh\n",
        "output = 'dataset.zip'\n",
        "\n",
        "# Mengunduh file\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Mengekstrak file zip jika diperlukan\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall('datasets/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlZl76MQ-V_7"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDD5bqhi9hfA"
      },
      "source": [
        "## Load and Clean Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB3o5rJJ9f80"
      },
      "outputs": [],
      "source": [
        "def check_and_remove_invalid_images(root_dir):\n",
        "    invalid_files = []\n",
        "    for subdir, dirs, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(subdir, file)\n",
        "            try:\n",
        "                with Image.open(file_path) as img:\n",
        "                    img.verify()  # Attempt to open and verify the image\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image: {file_path} - {e}\")\n",
        "                invalid_files.append(file_path)\n",
        "    # Remove invalid files\n",
        "    for invalid_file in invalid_files:\n",
        "        os.remove(invalid_file)\n",
        "        print(f\"Invalid image removed: {invalid_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxCMyqc_mOlX"
      },
      "outputs": [],
      "source": [
        "def get_file_paths_and_labels(root_dir):\n",
        "    # Check and remove invalid images before collecting file paths and labels\n",
        "    check_and_remove_invalid_images(root_dir)\n",
        "\n",
        "    file_paths = []\n",
        "    labels = []\n",
        "    for subdir, dirs, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(subdir, file)\n",
        "            label = os.path.basename(subdir)\n",
        "            file_paths.append(file_path)\n",
        "            labels.append(label)\n",
        "    return file_paths, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_y3IZjSmREC",
        "outputId": "8727ec01-9d28-4ba8-cc10-25c57e9305f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing image: /content/datasets/daun kemangi/daun-kemangi_160.jpg - cannot identify image file '/content/datasets/daun kemangi/daun-kemangi_160.jpg'\n",
            "Error processing image: /content/datasets/daun kemangi/daun-kemangi_154.jpg - cannot identify image file '/content/datasets/daun kemangi/daun-kemangi_154.jpg'\n",
            "Error processing image: /content/datasets/daun kemangi/daun-kemangi_180.jpg - cannot identify image file '/content/datasets/daun kemangi/daun-kemangi_180.jpg'\n",
            "Error processing image: /content/datasets/bawang putih/bawang-putih_152.jpg - cannot identify image file '/content/datasets/bawang putih/bawang-putih_152.jpg'\n",
            "Invalid image removed: /content/datasets/daun kemangi/daun-kemangi_160.jpg\n",
            "Invalid image removed: /content/datasets/daun kemangi/daun-kemangi_154.jpg\n",
            "Invalid image removed: /content/datasets/daun kemangi/daun-kemangi_180.jpg\n",
            "Invalid image removed: /content/datasets/bawang putih/bawang-putih_152.jpg\n"
          ]
        }
      ],
      "source": [
        "# Direktori asal, train, dan validation\n",
        "root_dir = '/content/datasets/'\n",
        "train_dir = '/content/final-datasets/train'\n",
        "val_dir = '/content/final-datasets/valid'\n",
        "\n",
        "# Membuat direktori train dan validation jika belum ada\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "# Mendapatkan semua jalur file dan labelnya\n",
        "file_paths, labels = get_file_paths_and_labels(root_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcaT8pQV92s5"
      },
      "source": [
        "## Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3bL_r-x-Cqj"
      },
      "outputs": [],
      "source": [
        "def split_and_move_files(file_paths, labels, train_dir, val_dir, test_size=0.2):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(file_paths, labels, test_size=test_size, stratify=labels, random_state=234)\n",
        "\n",
        "    for file_path, label in zip(X_train, y_train):\n",
        "        label_dir = os.path.join(train_dir, label)\n",
        "        os.makedirs(label_dir, exist_ok=True)\n",
        "        shutil.copy(file_path, label_dir)\n",
        "\n",
        "    for file_path, label in zip(X_val, y_val):\n",
        "        label_dir = os.path.join(val_dir, label)\n",
        "        os.makedirs(label_dir, exist_ok=True)\n",
        "        shutil.copy(file_path, label_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7_nRT5v9-Hj"
      },
      "outputs": [],
      "source": [
        "# Membagi dan memindahkan file ke direktori train dan validation\n",
        "split_and_move_files(\n",
        "    file_paths,\n",
        "    labels,\n",
        "    train_dir,\n",
        "    val_dir,\n",
        "    test_size=0.2\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zXAtkvf9EXy"
      },
      "source": [
        "## Data Generator and Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4QTDVnw803t",
        "outputId": "44548b41-c2a8-424a-d516-c5a3780b5b42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5049 images belonging to 31 classes.\n",
            "Found 1263 images belonging to 31 classes.\n"
          ]
        }
      ],
      "source": [
        "# ImageDataGenerator untuk training dan validation\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "\t    rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Membuat generator untuk training dan validation\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jZDbRE9Bsx2"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsyPvY5rmTH8",
        "outputId": "f04bc900-9270-4cb4-dcbb-15734fcfd98c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681ms/step - accuracy: 0.0495 - loss: 3.6499"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 746ms/step - accuracy: 0.0497 - loss: 3.6479 - val_accuracy: 0.0974 - val_loss: 3.4264\n",
            "Epoch 2/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 694ms/step - accuracy: 0.1722 - loss: 2.8178 - val_accuracy: 0.2423 - val_loss: 2.4650\n",
            "Epoch 3/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 698ms/step - accuracy: 0.2603 - loss: 2.4823 - val_accuracy: 0.3508 - val_loss: 2.0517\n",
            "Epoch 4/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 690ms/step - accuracy: 0.3235 - loss: 2.1928 - val_accuracy: 0.4252 - val_loss: 1.7738\n",
            "Epoch 5/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 689ms/step - accuracy: 0.3918 - loss: 1.9498 - val_accuracy: 0.4181 - val_loss: 1.8643\n",
            "Epoch 6/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 682ms/step - accuracy: 0.4181 - loss: 1.8535 - val_accuracy: 0.4173 - val_loss: 1.9714\n",
            "Epoch 7/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 693ms/step - accuracy: 0.4498 - loss: 1.7436 - val_accuracy: 0.5234 - val_loss: 1.5494\n",
            "Epoch 8/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 676ms/step - accuracy: 0.4898 - loss: 1.6448 - val_accuracy: 0.5337 - val_loss: 1.4836\n",
            "Epoch 9/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 674ms/step - accuracy: 0.4991 - loss: 1.5753 - val_accuracy: 0.5598 - val_loss: 1.4070\n",
            "Epoch 10/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 668ms/step - accuracy: 0.5280 - loss: 1.5089 - val_accuracy: 0.4291 - val_loss: 1.9241\n",
            "Epoch 11/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 689ms/step - accuracy: 0.5323 - loss: 1.4816 - val_accuracy: 0.5622 - val_loss: 1.5239\n",
            "Epoch 12/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 704ms/step - accuracy: 0.5537 - loss: 1.3654 - val_accuracy: 0.6215 - val_loss: 1.2262\n",
            "Epoch 13/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 720ms/step - accuracy: 0.5755 - loss: 1.3387 - val_accuracy: 0.5740 - val_loss: 1.4230\n",
            "Epoch 14/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 689ms/step - accuracy: 0.5893 - loss: 1.3197 - val_accuracy: 0.5637 - val_loss: 1.5061\n",
            "Epoch 15/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 722ms/step - accuracy: 0.6019 - loss: 1.2712 - val_accuracy: 0.6564 - val_loss: 1.1569\n",
            "Epoch 16/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 697ms/step - accuracy: 0.6155 - loss: 1.2314 - val_accuracy: 0.6247 - val_loss: 1.3054\n",
            "Epoch 17/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 727ms/step - accuracy: 0.6265 - loss: 1.1976 - val_accuracy: 0.6849 - val_loss: 1.1244\n",
            "Epoch 18/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 687ms/step - accuracy: 0.6280 - loss: 1.1625 - val_accuracy: 0.5962 - val_loss: 1.3844\n",
            "Epoch 19/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 693ms/step - accuracy: 0.6743 - loss: 1.0429 - val_accuracy: 0.6279 - val_loss: 1.2586\n",
            "Epoch 20/20\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 709ms/step - accuracy: 0.6720 - loss: 1.0438 - val_accuracy: 0.6508 - val_loss: 1.2586\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b4c3c34e230>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    Conv2D(64, (3,3), activation='relu', input_shape=(256, 256, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dropout(0.5),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(31, activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = 'rmsprop'\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=20,\n",
        "    batch_size = 32,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
